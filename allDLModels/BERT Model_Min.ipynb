{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.python.keras import backend as K\n",
    "# config = tf.compat.v1.ConfigProto(device_count = {'GPU': 6, 'CPU' : 49} )\n",
    "# sess = tf.compat.v1.Session(config=config) \n",
    "# K.set_session(sess)\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>RECORD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>OC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>gallstone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>pancreatitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence          Word  Tag\n",
       "0  Sentence: 1        RECORD    0\n",
       "1  Sentence: 2            OC    0\n",
       "2          NaN            AM    0\n",
       "3          NaN     gallstone    0\n",
       "4          NaN  pancreatitis    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./ProcessedDataSet/dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Min\\Anaconda3\\envs\\symptom\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[RECORD]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[WILL, D/C, ORDER, BE, USED, AS, THE, D/C, SUM...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[prandial, N/V/severe, upper, abdominal, pain....</td>\n",
       "      <td>[0, 1, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[normal, limits., Cardiac, catheterization, da...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[year, old, Black, female, with, significant, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentence                                               Word  \\\n",
       "0      Sentence: 1                                           [RECORD]   \n",
       "1     Sentence: 10  [WILL, D/C, ORDER, BE, USED, AS, THE, D/C, SUM...   \n",
       "2    Sentence: 100  [prandial, N/V/severe, upper, abdominal, pain....   \n",
       "3   Sentence: 1000  [normal, limits., Cardiac, catheterization, da...   \n",
       "4  Sentence: 10000  [year, old, Black, female, with, significant, ...   \n",
       "\n",
       "                              Tag  \n",
       "0                             [0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 1, 0, 1, 1, 0, 0, 0, 0]  \n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_group = data_fillna.groupby(['Sentence'],as_index=False\n",
    "                                )['Word', 'Tag'].agg(lambda x: list(x))\n",
    "\n",
    "#data_fillna\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_group['Word'].tolist()  \n",
    "labels = data_group['Tag'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensAll = []\n",
    "tagsAll = []\n",
    "for tokenLine, tagLine in zip(texts, labels):\n",
    "    for token, tag in zip(tokenLine, tagLine):\n",
    "        tokensAll.append(token)\n",
    "        tagsAll.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n"
     ]
    }
   ],
   "source": [
    "indexSequence = 0\n",
    "tokens607 = []\n",
    "tags607 = []\n",
    "count = 0\n",
    "tokens = []\n",
    "tags = []\n",
    "firstTime = 0\n",
    "appended = 0\n",
    "while indexSequence < len(tokensAll):\n",
    "    if count != 512:\n",
    "        tokens.append(tokensAll[indexSequence])\n",
    "        tags.append(tagsAll[indexSequence])\n",
    "        indexSequence = indexSequence + 1\n",
    "        count = count + 1\n",
    "    elif count == 512 and firstTime == 0:\n",
    "        tokens607 = np.array(tokens)\n",
    "        tags607 = np.array(tags)\n",
    "        firstTime = 1\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        indexSequence = indexSequence-112\n",
    "        count = 0 \n",
    "        appended = 1\n",
    "        print(appended)\n",
    "    elif count == 512 and firstTime == 1:\n",
    "        tokens607 = np.vstack((tokens607, np.array(tokens)))\n",
    "        tags607 = np.vstack((tags607, np.array(tags)))\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        indexSequence = indexSequence-112\n",
    "        count = 0 \n",
    "        appended = appended + 1\n",
    "        print(appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stride</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stride: 1</td>\n",
       "      <td>[RECORD, WILL, D/C, ORDER, BE, USED, AS, THE, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stride: 2</td>\n",
       "      <td>[well, expanded., He, was, found, to, be, hypo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stride: 3</td>\n",
       "      <td>[EMSSten, Tel, Dictated, By:, QUARRY, FERNANDO...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stride: 4</td>\n",
       "      <td>[lymph, node., She, should, have, repeat, CT, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stride: 5</td>\n",
       "      <td>[room, air., The, patient, appeared, in, no, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Stride                                               Word  \\\n",
       "0  Stride: 1  [RECORD, WILL, D/C, ORDER, BE, USED, AS, THE, ...   \n",
       "1  Stride: 2  [well, expanded., He, was, found, to, be, hypo...   \n",
       "2  Stride: 3  [EMSSten, Tel, Dictated, By:, QUARRY, FERNANDO...   \n",
       "3  Stride: 4  [lymph, node., She, should, have, repeat, CT, ...   \n",
       "4  Stride: 5  [room, air., The, patient, appeared, in, no, a...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataBert = pd.DataFrame(columns = ['Stride', 'Word', 'Tag'])\n",
    "stride = 1\n",
    "count = 0\n",
    "while count < len(tokens607):\n",
    "    dataBert.loc[len(dataBert.index)] = ['Stride: ' + str(stride), tokens607[count], tags607[count]] \n",
    "    count = count + 1\n",
    "    stride = stride + 1\n",
    "\n",
    "dataBert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilbert-base-uncased\n",
    "# emilyalsentzer/Bio_ClinicalBERT\n",
    "BERT_MODEL = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(dataBert, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_train = Dataset.from_pandas(data_train)\n",
    "dataset_test = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"Word\"], truncation=True, is_split_into_words=True, padding = 'max_length', max_length =512)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"Tag\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                            # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:              # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0466ad5921d74261b7974823802a9287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665fac90812549b585f1d32c06ed4249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_train = dataset_train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data_test = dataset_test.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Stride', 'Tag', 'Word', '__index_level_0__', 'attention_mask', 'input_ids', 'labels', 'token_type_ids'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_train = tokenized_data_train.remove_columns(['Stride'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Stride'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['Word'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Word'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['Tag'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['Tag'])\n",
    "\n",
    "tokenized_data_train = tokenized_data_train.remove_columns(['__index_level_0__'])\n",
    "tokenized_data_test = tokenized_data_test.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding = 'max_length',  max_length =512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(BERT_MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"seqeval\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": all_metrics[\"overall_precision\"],\n",
    "#         \"recall\": all_metrics[\"overall_recall\"],\n",
    "#         \"f1\": all_metrics[\"overall_f1\"],\n",
    "#         \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    \n",
    "    all_pred = [p for ps in true_predictions for p in ps]\n",
    "    all_label = [l for ls in true_labels for l in ls]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_label, all_pred, average='binary')\n",
    "    acc = accuracy_score(all_label, all_pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./epochs',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps= 2000,\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1661\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 41500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8836' max='41500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8836/41500 1:45:59 < 6:31:56, 1.39 it/s, Epoch 21.29/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035968</td>\n",
       "      <td>0.987518</td>\n",
       "      <td>0.759981</td>\n",
       "      <td>0.787421</td>\n",
       "      <td>0.734389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>0.989388</td>\n",
       "      <td>0.792493</td>\n",
       "      <td>0.836246</td>\n",
       "      <td>0.753091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029365</td>\n",
       "      <td>0.990397</td>\n",
       "      <td>0.811019</td>\n",
       "      <td>0.861958</td>\n",
       "      <td>0.765764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029427</td>\n",
       "      <td>0.990163</td>\n",
       "      <td>0.809704</td>\n",
       "      <td>0.844430</td>\n",
       "      <td>0.777721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.990452</td>\n",
       "      <td>0.815907</td>\n",
       "      <td>0.847824</td>\n",
       "      <td>0.786306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.029223</td>\n",
       "      <td>0.990749</td>\n",
       "      <td>0.821538</td>\n",
       "      <td>0.854164</td>\n",
       "      <td>0.791313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.029203</td>\n",
       "      <td>0.990776</td>\n",
       "      <td>0.824948</td>\n",
       "      <td>0.842987</td>\n",
       "      <td>0.807665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.028185</td>\n",
       "      <td>0.990961</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>0.837523</td>\n",
       "      <td>0.823914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.990840</td>\n",
       "      <td>0.822621</td>\n",
       "      <td>0.858795</td>\n",
       "      <td>0.789371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.990375</td>\n",
       "      <td>0.819234</td>\n",
       "      <td>0.828130</td>\n",
       "      <td>0.810526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.031517</td>\n",
       "      <td>0.990490</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.833667</td>\n",
       "      <td>0.807767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.990317</td>\n",
       "      <td>0.821387</td>\n",
       "      <td>0.815471</td>\n",
       "      <td>0.827389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.038327</td>\n",
       "      <td>0.989665</td>\n",
       "      <td>0.806806</td>\n",
       "      <td>0.811731</td>\n",
       "      <td>0.801942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.988257</td>\n",
       "      <td>0.799775</td>\n",
       "      <td>0.738931</td>\n",
       "      <td>0.871538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.034910</td>\n",
       "      <td>0.989979</td>\n",
       "      <td>0.815344</td>\n",
       "      <td>0.808624</td>\n",
       "      <td>0.822177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>0.990303</td>\n",
       "      <td>0.814616</td>\n",
       "      <td>0.838874</td>\n",
       "      <td>0.791722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.038454</td>\n",
       "      <td>0.990719</td>\n",
       "      <td>0.821192</td>\n",
       "      <td>0.852585</td>\n",
       "      <td>0.792029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.039494</td>\n",
       "      <td>0.989536</td>\n",
       "      <td>0.812145</td>\n",
       "      <td>0.785578</td>\n",
       "      <td>0.840572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.989927</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.807948</td>\n",
       "      <td>0.820746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>0.990474</td>\n",
       "      <td>0.820239</td>\n",
       "      <td>0.833210</td>\n",
       "      <td>0.807665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>0.990314</td>\n",
       "      <td>0.818472</td>\n",
       "      <td>0.825621</td>\n",
       "      <td>0.811446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.044087</td>\n",
       "      <td>0.990543</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.856677</td>\n",
       "      <td>0.778845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.042587</td>\n",
       "      <td>0.990746</td>\n",
       "      <td>0.821967</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.793868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.989638</td>\n",
       "      <td>0.811581</td>\n",
       "      <td>0.794576</td>\n",
       "      <td>0.829331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.041551</td>\n",
       "      <td>0.990548</td>\n",
       "      <td>0.822661</td>\n",
       "      <td>0.830763</td>\n",
       "      <td>0.814716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>0.990642</td>\n",
       "      <td>0.822954</td>\n",
       "      <td>0.838173</td>\n",
       "      <td>0.808278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.042762</td>\n",
       "      <td>0.990081</td>\n",
       "      <td>0.816166</td>\n",
       "      <td>0.814050</td>\n",
       "      <td>0.818293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.043709</td>\n",
       "      <td>0.990776</td>\n",
       "      <td>0.823992</td>\n",
       "      <td>0.846834</td>\n",
       "      <td>0.802351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.042075</td>\n",
       "      <td>0.990427</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.832419</td>\n",
       "      <td>0.806643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.046489</td>\n",
       "      <td>0.990259</td>\n",
       "      <td>0.814594</td>\n",
       "      <td>0.834961</td>\n",
       "      <td>0.795197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.047516</td>\n",
       "      <td>0.990210</td>\n",
       "      <td>0.810699</td>\n",
       "      <td>0.845028</td>\n",
       "      <td>0.779050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.989762</td>\n",
       "      <td>0.810833</td>\n",
       "      <td>0.806285</td>\n",
       "      <td>0.815432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.048885</td>\n",
       "      <td>0.990213</td>\n",
       "      <td>0.817684</td>\n",
       "      <td>0.819741</td>\n",
       "      <td>0.815636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>0.811354</td>\n",
       "      <td>0.793859</td>\n",
       "      <td>0.829637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.049789</td>\n",
       "      <td>0.990658</td>\n",
       "      <td>0.823432</td>\n",
       "      <td>0.837846</td>\n",
       "      <td>0.809504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.052081</td>\n",
       "      <td>0.990422</td>\n",
       "      <td>0.818811</td>\n",
       "      <td>0.833863</td>\n",
       "      <td>0.804292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.050546</td>\n",
       "      <td>0.990116</td>\n",
       "      <td>0.815654</td>\n",
       "      <td>0.818762</td>\n",
       "      <td>0.812570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.051879</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.814233</td>\n",
       "      <td>0.792493</td>\n",
       "      <td>0.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.051903</td>\n",
       "      <td>0.990257</td>\n",
       "      <td>0.817831</td>\n",
       "      <td>0.822951</td>\n",
       "      <td>0.812775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>0.990339</td>\n",
       "      <td>0.812710</td>\n",
       "      <td>0.849532</td>\n",
       "      <td>0.778947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.052279</td>\n",
       "      <td>0.990034</td>\n",
       "      <td>0.817761</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>0.830966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>0.990312</td>\n",
       "      <td>0.816939</td>\n",
       "      <td>0.830973</td>\n",
       "      <td>0.803373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.046722</td>\n",
       "      <td>0.990628</td>\n",
       "      <td>0.820745</td>\n",
       "      <td>0.845562</td>\n",
       "      <td>0.797343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.052620</td>\n",
       "      <td>0.990394</td>\n",
       "      <td>0.819586</td>\n",
       "      <td>0.828530</td>\n",
       "      <td>0.810833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.055694</td>\n",
       "      <td>0.989610</td>\n",
       "      <td>0.810570</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.826060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.055139</td>\n",
       "      <td>0.990122</td>\n",
       "      <td>0.815814</td>\n",
       "      <td>0.818668</td>\n",
       "      <td>0.812979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.054922</td>\n",
       "      <td>0.990394</td>\n",
       "      <td>0.817130</td>\n",
       "      <td>0.837699</td>\n",
       "      <td>0.797547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.051831</td>\n",
       "      <td>0.990056</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.818877</td>\n",
       "      <td>0.809504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.053902</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>0.815728</td>\n",
       "      <td>0.842277</td>\n",
       "      <td>0.790802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>0.816880</td>\n",
       "      <td>0.826843</td>\n",
       "      <td>0.807154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.059011</td>\n",
       "      <td>0.990224</td>\n",
       "      <td>0.812589</td>\n",
       "      <td>0.839177</td>\n",
       "      <td>0.787634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>0.990367</td>\n",
       "      <td>0.818413</td>\n",
       "      <td>0.830423</td>\n",
       "      <td>0.806745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.990012</td>\n",
       "      <td>0.813686</td>\n",
       "      <td>0.816871</td>\n",
       "      <td>0.810526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.056046</td>\n",
       "      <td>0.990053</td>\n",
       "      <td>0.815053</td>\n",
       "      <td>0.815596</td>\n",
       "      <td>0.814512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.990232</td>\n",
       "      <td>0.818442</td>\n",
       "      <td>0.818693</td>\n",
       "      <td>0.818191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>0.821137</td>\n",
       "      <td>0.840377</td>\n",
       "      <td>0.802759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.989971</td>\n",
       "      <td>0.812136</td>\n",
       "      <td>0.818758</td>\n",
       "      <td>0.805621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.055996</td>\n",
       "      <td>0.990218</td>\n",
       "      <td>0.818140</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.817680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.811650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.056977</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>0.817484</td>\n",
       "      <td>0.824667</td>\n",
       "      <td>0.810424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.058473</td>\n",
       "      <td>0.990158</td>\n",
       "      <td>0.815905</td>\n",
       "      <td>0.821355</td>\n",
       "      <td>0.810526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.060196</td>\n",
       "      <td>0.990422</td>\n",
       "      <td>0.815412</td>\n",
       "      <td>0.846874</td>\n",
       "      <td>0.786203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.048035</td>\n",
       "      <td>0.989674</td>\n",
       "      <td>0.812615</td>\n",
       "      <td>0.794032</td>\n",
       "      <td>0.832090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.990158</td>\n",
       "      <td>0.818482</td>\n",
       "      <td>0.812424</td>\n",
       "      <td>0.824630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.056683</td>\n",
       "      <td>0.990754</td>\n",
       "      <td>0.823257</td>\n",
       "      <td>0.847678</td>\n",
       "      <td>0.800204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.060135</td>\n",
       "      <td>0.990471</td>\n",
       "      <td>0.820606</td>\n",
       "      <td>0.831584</td>\n",
       "      <td>0.809913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.060285</td>\n",
       "      <td>0.989984</td>\n",
       "      <td>0.810510</td>\n",
       "      <td>0.825543</td>\n",
       "      <td>0.796014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>0.989872</td>\n",
       "      <td>0.810905</td>\n",
       "      <td>0.814796</td>\n",
       "      <td>0.807052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.053320</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.828331</td>\n",
       "      <td>0.803781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.990268</td>\n",
       "      <td>0.819060</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.054278</td>\n",
       "      <td>0.990166</td>\n",
       "      <td>0.817103</td>\n",
       "      <td>0.817856</td>\n",
       "      <td>0.816352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.058825</td>\n",
       "      <td>0.989819</td>\n",
       "      <td>0.813087</td>\n",
       "      <td>0.803513</td>\n",
       "      <td>0.822892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.990306</td>\n",
       "      <td>0.819758</td>\n",
       "      <td>0.820303</td>\n",
       "      <td>0.819213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.061442</td>\n",
       "      <td>0.990650</td>\n",
       "      <td>0.821709</td>\n",
       "      <td>0.843834</td>\n",
       "      <td>0.800715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.053211</td>\n",
       "      <td>0.990276</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>0.822147</td>\n",
       "      <td>0.814921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.990364</td>\n",
       "      <td>0.818727</td>\n",
       "      <td>0.829020</td>\n",
       "      <td>0.808687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.054658</td>\n",
       "      <td>0.990015</td>\n",
       "      <td>0.814698</td>\n",
       "      <td>0.813660</td>\n",
       "      <td>0.815738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>0.990075</td>\n",
       "      <td>0.812490</td>\n",
       "      <td>0.826358</td>\n",
       "      <td>0.799080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.048957</td>\n",
       "      <td>0.990328</td>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.828925</td>\n",
       "      <td>0.807154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.990317</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.815408</td>\n",
       "      <td>0.827491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.055656</td>\n",
       "      <td>0.990320</td>\n",
       "      <td>0.818238</td>\n",
       "      <td>0.826949</td>\n",
       "      <td>0.809709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.052278</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>0.815294</td>\n",
       "      <td>0.832623</td>\n",
       "      <td>0.798671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.057967</td>\n",
       "      <td>0.990655</td>\n",
       "      <td>0.822057</td>\n",
       "      <td>0.842981</td>\n",
       "      <td>0.802146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.989852</td>\n",
       "      <td>0.808471</td>\n",
       "      <td>0.821432</td>\n",
       "      <td>0.795912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.058006</td>\n",
       "      <td>0.990369</td>\n",
       "      <td>0.817395</td>\n",
       "      <td>0.834451</td>\n",
       "      <td>0.801022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.057986</td>\n",
       "      <td>0.990345</td>\n",
       "      <td>0.818356</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.808278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.990314</td>\n",
       "      <td>0.818641</td>\n",
       "      <td>0.825013</td>\n",
       "      <td>0.812366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.055460</td>\n",
       "      <td>0.989624</td>\n",
       "      <td>0.806125</td>\n",
       "      <td>0.810666</td>\n",
       "      <td>0.801635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-2000\n",
      "Configuration saved in ./epochs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-4000\n",
      "Configuration saved in ./epochs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-6000\n",
      "Configuration saved in ./epochs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-6000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./epochs\\checkpoint-8000\n",
      "Configuration saved in ./epochs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./epochs\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./epochs\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./epochs\\checkpoint-8000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 713\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data_train,\n",
    "    eval_dataset=tokenized_data_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_data_test)\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    \n",
    "    all_pred = [p for ps in true_predictions for p in ps]\n",
    "    all_label = [l for ls in true_labels for l in ls]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_label, all_pred, average='binary')\n",
    "    acc = accuracy_score(all_label, all_pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
